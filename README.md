# TLR
Our proposed multi-attention based transformer learning model captures visitor’s historical check-ins spatiotemporal dependencies for POI recommendation.

To use this code in your research work please cite the following paper.

Sajal Halder, Kwan Hui Lim, Jeﬀrey Chan, and Xiuzhen Zhang. Transformer-based multi-task learning for queuing time aware next poi recommendation. In Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 510–523. Springer, 2021

# Implemtation Details
In this TLR model implemenation, we have used transformer based attention machanism that has been implemented in python programing language. We use tensorflow, keras and attention machanism. 


